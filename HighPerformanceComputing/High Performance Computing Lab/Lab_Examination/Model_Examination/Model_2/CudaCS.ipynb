{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctf3QrLXS3Dv"
   },
   "source": [
    "# CUDA C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fPDcYJIBJkhd"
   },
   "source": [
    "GPT : https://chatgpt.com/share/6728f8c2-7aac-8002-b253-397bfeb02832"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0OT3NJwS9ei"
   },
   "source": [
    "For writing CUDA program write this in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cqFNfjhhSzPG",
    "outputId": "347468b7-2322-4ace-d763-db2ec1bece32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov  4 19:42:17 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   56C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vdF-vnWITanN",
    "outputId": "44124c2f-5310-4272-c0c9-769f2b4846f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
      "Cuda compilation tools, release 12.2, V12.2.140\n",
      "Build cuda_12.2.r12.2/compiler.33191640_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mq9EaGLuTkne",
    "outputId": "f1b74c3b-1b56-42f7-b90f-1cf1e010359c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'cuda-samples'...\n",
      "remote: Enumerating objects: 19507, done.\u001b[K\n",
      "remote: Counting objects: 100% (4922/4922), done.\u001b[K\n",
      "remote: Compressing objects: 100% (658/658), done.\u001b[K\n",
      "remote: Total 19507 (delta 4565), reused 4336 (delta 4264), pack-reused 14585 (from 1)\u001b[K\n",
      "Receiving objects: 100% (19507/19507), 133.38 MiB | 11.91 MiB/s, done.\n",
      "Resolving deltas: 100% (17225/17225), done.\n",
      "Updating files: 100% (4026/4026), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/NVIDIA/cuda-samples.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S3X-1RBLT0gd",
    "outputId": "9a14e869-0e64-4dcd-d745-da1773206b41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 --threads 0 --std=c++11 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o deviceQuery.o -c deviceQuery.cpp\n",
      "/usr/local/cuda/bin/nvcc -ccbin g++ -m64 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o deviceQuery deviceQuery.o \n",
      "mkdir -p ../../../bin/x86_64/linux/release\n",
      "cp deviceQuery ../../../bin/x86_64/linux/release\n"
     ]
    }
   ],
   "source": [
    "!cd cuda-samples/Samples/1_Utilities/deviceQuery &&make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y6Twr_nBVHH_",
    "outputId": "99361a2c-37b0-463a-ab4a-3455b17bdbf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deviceQuery\t deviceQuery_vs2017.sln      deviceQuery_vs2019.vcxproj  Makefile\n",
      "deviceQuery.cpp  deviceQuery_vs2017.vcxproj  deviceQuery_vs2022.sln\t NsightEclipse.xml\n",
      "deviceQuery.o\t deviceQuery_vs2019.sln      deviceQuery_vs2022.vcxproj  README.md\n",
      "cuda-samples/Samples/1_Utilities/deviceQuery/./deviceQuery Starting...\n",
      "\n",
      " CUDA Device Query (Runtime API) version (CUDART static linking)\n",
      "\n",
      "Detected 1 CUDA Capable device(s)\n",
      "\n",
      "Device 0: \"Tesla T4\"\n",
      "  CUDA Driver Version / Runtime Version          12.2 / 12.2\n",
      "  CUDA Capability Major/Minor version number:    7.5\n",
      "  Total amount of global memory:                 15102 MBytes (15835660288 bytes)\n",
      "  (040) Multiprocessors, (064) CUDA Cores/MP:    2560 CUDA Cores\n",
      "  GPU Max Clock rate:                            1590 MHz (1.59 GHz)\n",
      "  Memory Clock rate:                             5001 Mhz\n",
      "  Memory Bus Width:                              256-bit\n",
      "  L2 Cache Size:                                 4194304 bytes\n",
      "  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\n",
      "  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\n",
      "  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\n",
      "  Total amount of constant memory:               65536 bytes\n",
      "  Total amount of shared memory per block:       49152 bytes\n",
      "  Total shared memory per multiprocessor:        65536 bytes\n",
      "  Total number of registers available per block: 65536\n",
      "  Warp size:                                     32\n",
      "  Maximum number of threads per multiprocessor:  1024\n",
      "  Maximum number of threads per block:           1024\n",
      "  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n",
      "  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n",
      "  Maximum memory pitch:                          2147483647 bytes\n",
      "  Texture alignment:                             512 bytes\n",
      "  Concurrent copy and kernel execution:          Yes with 3 copy engine(s)\n",
      "  Run time limit on kernels:                     No\n",
      "  Integrated GPU sharing Host Memory:            No\n",
      "  Support host page-locked memory mapping:       Yes\n",
      "  Alignment requirement for Surfaces:            Yes\n",
      "  Device has ECC support:                        Enabled\n",
      "  Device supports Unified Addressing (UVA):      Yes\n",
      "  Device supports Managed Memory:                Yes\n",
      "  Device supports Compute Preemption:            Yes\n",
      "  Supports Cooperative Kernel Launch:            Yes\n",
      "  Supports MultiDevice Co-op Kernel Launch:      Yes\n",
      "  Device PCI Domain ID / Bus ID / location ID:   0 / 0 / 4\n",
      "  Compute Mode:\n",
      "     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n",
      "\n",
      "deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 12.2, CUDA Runtime Version = 12.2, NumDevs = 1\n",
      "Result = PASS\n"
     ]
    }
   ],
   "source": [
    "!cd cuda-samples/Samples/1_Utilities/deviceQuery &&ls\n",
    "!cuda-samples/Samples/1_Utilities/deviceQuery/./deviceQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A0p9GzB2Vcq5",
    "outputId": "55fee2c9-6cdb-45cb-f2d5-0874c7d7bf0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nvcc4jupyter\n",
      "  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: nvcc4jupyter\n",
      "Successfully installed nvcc4jupyter-1.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nvcc4jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rFCMb7aTVh0K",
    "outputId": "f99b3bbf-6491-43dc-a697-eaa4c2a9f314"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected platform \"Colab\". Running its setup...\n",
      "Source files will be saved in \"/tmp/tmph373i958\".\n"
     ]
    }
   ],
   "source": [
    "%load_ext nvcc4jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jaGDyJQZXFc"
   },
   "source": [
    "1. Hello World Program (helloWorld.cu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sdGKLCtXT6AC",
    "outputId": "213c2e0f-4d57-4fa9-cff8-30a79ec8a5e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing helloWorld.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile helloWorld.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda.h>\n",
    "\n",
    "const int a = 5;\n",
    "\n",
    "__global__ void my_kernel() {\n",
    "  printf(\"Hello, World! <-- GPU\\n\");\n",
    "  printf(\"Value printed from GPU: %d\\n\", a);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "  printf(\"Hello, World! <-- CPU\\n\" );\n",
    "  my_kernel<<<1, 1>>>();\n",
    "  printf(\"Value printed from GPU: %d\\n\", a);\n",
    "  cudaDeviceSynchronize();\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fZvbZtk4Uirb",
    "outputId": "faca0912-af32-4535-ce47-f549d5c8ffdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World! <-- CPU\n",
      "Value printed from GPU: 5\n",
      "Hello, World! <-- GPU\n",
      "Value printed from GPU: 5\n"
     ]
    }
   ],
   "source": [
    "!nvcc helloWorld.cu -o helloWorld\n",
    "!./helloWorld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmJCjk3RaEO8"
   },
   "source": [
    "## Steps for CUDA Programming:\n",
    "\n",
    "- Load data to CPU memory\n",
    "- Copy data from CPU to GPU memory\n",
    "- Execute GPU kernel\n",
    "- Copy results from CPU to GPU memory\n",
    "- Use results on CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxV83BIVbctM"
   },
   "source": [
    "2. Write a CUDA Program to initialize an array of size N to all zeros in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UOb4pUmcZq5d"
   },
   "source": [
    "## Recommended Launch Configuration:\n",
    "\n",
    "**Choose a Number of Threads per Block:** A common choice is to use 256 or 512 threads per block. This number is a good balance between maximizing GPU occupancy and avoiding performance penalties associated with too many threads in a single block.\n",
    "\n",
    "**Calculate the Number of Blocks:** To calculate the number of blocks needed, use the formula:\n",
    "\n",
    "`numBlocks = 𝑁 + (threadsPerBlock − 1) / threadsPerBlock`\n",
    "\n",
    "\n",
    "This formula ensures that you cover all elements in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ftQ67FjWb5PT",
    "outputId": "28dccb0f-5c8d-4d64-b658-64593f410e6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing arrayInit.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile arrayInit.cu\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <cuda.h>\n",
    "\n",
    "__global__ void initArray(int N, int *arr) {\n",
    "  int id = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  if (id < N) { // Boundary check\n",
    "    arr[id] = 0;\n",
    "  }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "  int N = 10;\n",
    "  int *arr;\n",
    "  cudaMallocManaged(&arr, N * sizeof(int));\n",
    "\n",
    "  // Launching the kernel with 1 block of N threads -- Sample launch (We'll see more about launch in next question)\n",
    "  initArray<<<1, N>>>(N, arr);\n",
    "  cudaDeviceSynchronize();\n",
    "\n",
    "  // Printing the initialized array to confirm results\n",
    "  for (int i = 0; i < N; i++) {\n",
    "    printf(\"arr[%d] = %d\\n\", i, arr[i]);\n",
    "  }\n",
    "\n",
    "  // Free the allocated memory\n",
    "  cudaFree(arr);\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CKxUchQ3insg",
    "outputId": "ab95a626-3439-48a5-8083-e209b11256d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arr[0] = 0\n",
      "arr[1] = 0\n",
      "arr[2] = 0\n",
      "arr[3] = 0\n",
      "arr[4] = 0\n",
      "arr[5] = 0\n",
      "arr[6] = 0\n",
      "arr[7] = 0\n",
      "arr[8] = 0\n",
      "arr[9] = 0\n"
     ]
    }
   ],
   "source": [
    "!nvcc arrayInit.cu -o arrayInit\n",
    "!./arrayInit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhkNp738jN0p"
   },
   "source": [
    "3. Add a scalar to all elements of array. (Compare CPU and GPU time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SYVUd06DjV7F",
    "outputId": "dd497579-e84d-48ef-eade-c4d22ede7656"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing scalarAdd.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile scalarAdd.cu\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <cuda.h>\n",
    "#include <time.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "__global__ void scalarAdd_GPU(int *arr, int scalar, int N) {\n",
    "    int id = blockIdx.x * blockDim.x + threadIdx.x; // Correct variable name for id\n",
    "    if (id < N) {\n",
    "      arr[id] += scalar;\n",
    "    }\n",
    "}\n",
    "\n",
    "int *scalarAdd_CPU(int *arr, int scalar, int N) {\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        arr[i] += scalar;\n",
    "    }\n",
    "    return arr;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int N = 100000; // Change this to a larger number for better performance testing\n",
    "    int *arr;\n",
    "    arr = (int *)malloc(N * sizeof(int)); // Added missing semicolon\n",
    "\n",
    "    srand(time(NULL));\n",
    "    for (int i = 0; i < N; i++) { // Use N instead of 1000 for the loop\n",
    "        arr[i] = rand() % 100;\n",
    "    }\n",
    "\n",
    "    int scalar = 5;\n",
    "    int *arr_GPU;\n",
    "    cudaMallocManaged(&arr_GPU, N * sizeof(int));\n",
    "    cudaMemcpy(arr_GPU, arr, N * sizeof(int), cudaMemcpyHostToDevice);\n",
    "\n",
    "    // CPU timing\n",
    "    printf(\"CPU time: \\n\");\n",
    "    clock_t start = clock();\n",
    "    int *arr_CPU = scalarAdd_CPU(arr, scalar, N);\n",
    "    clock_t end = clock();\n",
    "    printf(\"Time elapsed: %f seconds\\n\\n\", (double)(end - start) / CLOCKS_PER_SEC);\n",
    "\n",
    "\n",
    "    // Launch configuration\n",
    "    int threadsPerBlock = 256; // Number of threads per block\n",
    "    int numBlocks = (N + threadsPerBlock - 1) / threadsPerBlock; // Calculate number of blocks\n",
    "\n",
    "    // GPU timing\n",
    "    printf(\"GPU time: \\n\");\n",
    "    cudaEvent_t startEvent, endEvent; // Renamed to avoid conflict with variables\n",
    "    cudaEventCreate(&startEvent);\n",
    "    cudaEventCreate(&endEvent);\n",
    "    cudaEventRecord(startEvent);\n",
    "\n",
    "    scalarAdd_GPU<<<numBlocks, threadsPerBlock>>>(arr_GPU, scalar, N); // Launching with multiple threads\n",
    "\n",
    "    cudaEventRecord(endEvent);\n",
    "    cudaEventSynchronize(endEvent); // Wait for the end event to complete\n",
    "\n",
    "    float milliseconds = 0;\n",
    "    cudaEventElapsedTime(&milliseconds, startEvent, endEvent); // Get elapsed time\n",
    "    printf(\"Time elapsed: %f seconds\\n\\n\", milliseconds / 1000.0); // Convert to seconds\n",
    "\n",
    "    // Freeing memory\n",
    "    cudaFree(arr_GPU);\n",
    "    free(arr);\n",
    "    cudaEventDestroy(startEvent);\n",
    "    cudaEventDestroy(endEvent);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z8PhEylLuvx0",
    "outputId": "6a7be11d-9cf7-4ed6-ed57-c85aa694ddeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: \n",
      "Time elapsed: 0.000315 seconds\n",
      "\n",
      "GPU time: \n",
      "Time elapsed: 0.057281 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nvcc scalarAdd.cu -o scalarAdd\n",
    "!./scalarAdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MnVpfziiQJcZ"
   },
   "source": [
    "**Matrix -> Vector representation:**\n",
    "\n",
    "M_mat = [[c0, c1, c2, c3], <br>\n",
    "[c4, c5, c6, c7], <br>\n",
    "[c8, c9, c10, c11], <br>\n",
    "[c12, c13, c14, c15]]\n",
    "\n",
    "is reresented as\n",
    "M_vec = [c0, c1, ......., c15]\n",
    "\n",
    "`M_vec[row * num_cols + col] = M_mat[row][col]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hw71Vs3IU5_8"
   },
   "source": [
    "4. Transpose a Matrix of size N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2rZaSOsZVAnu",
    "outputId": "82356f9e-7fa5-4598-c3a3-25094cfe4955"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing matrixTranpose.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile matrixTranpose.cu\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <cuda.h>\n",
    "\n",
    "__global__ void transpose(int *M_in, int *M_out, int rows, int cols) {\n",
    "    int id = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    // Calculate row and column based on the 1D index\n",
    "    int rowIdx = id / cols;\n",
    "    int colIdx = id % cols;\n",
    "\n",
    "    if (rowIdx < rows && colIdx < cols) {\n",
    "        // Perform transpose by swapping rows and columns\n",
    "        M_out[colIdx * rows + rowIdx] = M_in[rowIdx * cols + colIdx];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int rows = 1000;\n",
    "    int cols = 1000;\n",
    "    int N = rows * cols;\n",
    "    int M_in[N], M_out[N];\n",
    "\n",
    "    // Initialize the input matrix (flattened as a vector)\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        M_in[i] = i + 1;\n",
    "    }\n",
    "\n",
    "    int *d_M_in, *d_M_out;\n",
    "    cudaMalloc(&d_M_in, N * sizeof(int));\n",
    "    cudaMalloc(&d_M_out, N * sizeof(int));\n",
    "\n",
    "    cudaMemcpy(d_M_in, M_in, N * sizeof(int), cudaMemcpyHostToDevice);\n",
    "\n",
    "    // Launch configuration\n",
    "    int threadsPerBlock = 256;\n",
    "    int numBlocks = (N + threadsPerBlock - 1) / threadsPerBlock;\n",
    "\n",
    "    // Call the kernel\n",
    "    transpose<<<numBlocks, threadsPerBlock>>>(d_M_in, d_M_out, rows, cols);\n",
    "\n",
    "    // Copy result back to host\n",
    "    cudaMemcpy(M_out, d_M_out, N * sizeof(int), cudaMemcpyDeviceToHost);\n",
    "\n",
    "    // Print result\n",
    "    printf(\"Transposed Matrix:\\n\");\n",
    "    for (int i = 0; i < rows; i++) {\n",
    "        for (int j = 0; j < cols; j++) {\n",
    "            printf(\"%d \", M_out[i * cols + j]);\n",
    "        }\n",
    "        printf(\"\\n\");\n",
    "    }\n",
    "\n",
    "    cudaFree(d_M_in);\n",
    "    cudaFree(d_M_out);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WIOQF3cWYtvX",
    "outputId": "7a5dbeb3-9a9a-4532-9946-a20b28601ed9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transposed Matrix:\n",
      "1 5 9 13 \n",
      "2 6 10 14 \n",
      "3 7 11 15 \n",
      "4 8 12 16 \n"
     ]
    }
   ],
   "source": [
    "!nvcc matrixTranpose.cu -o matrixTranpose\n",
    "!./matrixTranpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jLx-CfZpdD_Y"
   },
   "source": [
    "5. Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6DLY1fi9dHmX",
    "outputId": "0bb3abf7-cdf4-4ca2-a9b7-12d5fe055b0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing matMult.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile matMult.cu\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <cuda.h>\n",
    "\n",
    "__global__ void matrixMultiply(int *A, int *B, int *C, int M, int K, int N) {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (row < M && col < N) {\n",
    "        int sum = 0;\n",
    "        for (int k = 0; k < K; k++) {\n",
    "            sum += A[row * K + k] * B[k * N + col];\n",
    "        }\n",
    "        C[row * N + col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int M = 4, K = 4, N = 4;\n",
    "    int sizeA = M * K;\n",
    "    int sizeB = K * N;\n",
    "    int sizeC = M * N;\n",
    "\n",
    "    int A[sizeA], B[sizeB], C[sizeC];\n",
    "\n",
    "    // Initialize matrices A and B with random values\n",
    "    for (int i = 0; i < sizeA; i++) A[i] = rand() % 10;\n",
    "    for (int i = 0; i < sizeB; i++) B[i] = rand() % 10;\n",
    "\n",
    "    int *d_A, *d_B, *d_C;\n",
    "    cudaMalloc(&d_A, sizeA * sizeof(int));\n",
    "    cudaMalloc(&d_B, sizeB * sizeof(int));\n",
    "    cudaMalloc(&d_C, sizeC * sizeof(int));\n",
    "\n",
    "    cudaMemcpy(d_A, A, sizeA * sizeof(int), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_B, B, sizeB * sizeof(int), cudaMemcpyHostToDevice);\n",
    "\n",
    "    // Configure grid and block dimensions\n",
    "    dim3 threadsPerBlock(16, 16); // 16x16 threads per block\n",
    "    dim3 numBlocks((N + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
    "                   (M + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
    "\n",
    "    // Launch kernel\n",
    "    matrixMultiply<<<numBlocks, threadsPerBlock>>>(d_A, d_B, d_C, M, K, N);\n",
    "\n",
    "    cudaMemcpy(C, d_C, sizeC * sizeof(int), cudaMemcpyDeviceToHost);\n",
    "\n",
    "    // Display result\n",
    "    printf(\"Matrix C (Result):\\n\");\n",
    "    for (int i = 0; i < M; i++) {\n",
    "        for (int j = 0; j < N; j++) {\n",
    "            printf(\"%d \", C[i * N + j]);\n",
    "        }\n",
    "        printf(\"\\n\");\n",
    "    }\n",
    "\n",
    "    cudaFree(d_A);\n",
    "    cudaFree(d_B);\n",
    "    cudaFree(d_C);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0MGwJi55dM5q",
    "outputId": "c9f12398-3bde-427d-a736-873102789a2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix C (Result):\n",
      "55 91 107 103 \n",
      "31 68 71 85 \n",
      "54 97 92 83 \n",
      "57 102 123 102 \n"
     ]
    }
   ],
   "source": [
    "!nvcc matMult.cu -o matMult\n",
    "!./matMult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1dYV1Z3przY2",
    "outputId": "3ae96926-d1fe-4d87-8ff4-c91af55542dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing matrixVectorScale.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile matrixVectorScale.cu\n",
    "\n",
    "#include <cuda.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "// CUDA kernel for matrix to vector conversion and element-wise scaling\n",
    "__global__ void matrixToVectorAndScale(const float* matrix, float* vector, float scale, int rows, int cols) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int size = rows * cols;\n",
    "\n",
    "    if (idx < size) {\n",
    "        vector[idx] = matrix[idx] * scale;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    // Example sizes\n",
    "    int rows = 1 << 10; // 1024\n",
    "    int cols = 1 << 10; // 1024\n",
    "    int size = rows * cols;\n",
    "\n",
    "    // Allocate host memory\n",
    "    float *h_matrix = (float*)malloc(size * sizeof(float));\n",
    "    float *h_vector = (float*)malloc(size * sizeof(float));\n",
    "\n",
    "    // Initialize matrix\n",
    "    for(int i = 0; i < size; i++) h_matrix[i] = static_cast<float>(i);\n",
    "\n",
    "    // Allocate device memory\n",
    "    float *d_matrix, *d_vector;\n",
    "    cudaMalloc((void**)&d_matrix, size * sizeof(float));\n",
    "    cudaMalloc((void**)&d_vector, size * sizeof(float));\n",
    "\n",
    "    // Copy data to device\n",
    "    cudaMemcpy(d_matrix, h_matrix, size * sizeof(float), cudaMemcpyHostToDevice);\n",
    "\n",
    "    // Define block and grid sizes\n",
    "    int threadsPerBlock = 256;\n",
    "    int blocksPerGrid = (size + threadsPerBlock - 1) / threadsPerBlock;\n",
    "\n",
    "    // Launch kernel with scaling factor, e.g., scale by 2.0\n",
    "    float scale = 2.0f;\n",
    "    matrixToVectorAndScale<<<blocksPerGrid, threadsPerBlock>>>(d_matrix, d_vector, scale, rows, cols);\n",
    "\n",
    "    // Copy result back to host\n",
    "    cudaMemcpy(h_vector, d_vector, size * sizeof(float), cudaMemcpyDeviceToHost);\n",
    "\n",
    "    // Cleanup\n",
    "    cudaFree(d_matrix);\n",
    "    cudaFree(d_vector);\n",
    "    free(h_matrix);\n",
    "    free(h_vector);\n",
    "\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVqWuBB8sA6k"
   },
   "outputs": [],
   "source": [
    "!nvcc matrixVectorScale.cu -o matrixVectorScale\n",
    "!./matrixVectorScale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K3ToMXUksd5F",
    "outputId": "c05d1b8b-0ecc-4f86-d49e-ba81710d5976"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==792== NVPROF is profiling process 792, command: ./matrixVectorScale\n",
      "==792== Profiling application: ./matrixVectorScale\n",
      "==792== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:   70.24%  1.9928ms         1  1.9928ms  1.9928ms  1.9928ms  [CUDA memcpy DtoH]\n",
      "                   28.32%  803.60us         1  803.60us  803.60us  803.60us  [CUDA memcpy HtoD]\n",
      "                    1.43%  40.703us         1  40.703us  40.703us  40.703us  matrixToVectorAndScale(float const *, float*, float, int, int)\n",
      "      API calls:   94.11%  88.741ms         2  44.370ms  120.85us  88.620ms  cudaMalloc\n",
      "                    5.01%  4.7233ms         2  2.3616ms  977.37us  3.7459ms  cudaMemcpy\n",
      "                    0.38%  357.18us         2  178.59us  134.56us  222.62us  cudaFree\n",
      "                    0.26%  245.47us         1  245.47us  245.47us  245.47us  cudaLaunchKernel\n",
      "                    0.19%  182.47us       114  1.6000us     190ns  71.371us  cuDeviceGetAttribute\n",
      "                    0.03%  31.712us         1  31.712us  31.712us  31.712us  cuDeviceGetName\n",
      "                    0.01%  7.6620us         1  7.6620us  7.6620us  7.6620us  cuDeviceGetPCIBusId\n",
      "                    0.01%  6.0320us         1  6.0320us  6.0320us  6.0320us  cuDeviceTotalMem\n",
      "                    0.00%  2.0720us         3     690ns     359ns  1.3520us  cuDeviceGetCount\n",
      "                    0.00%  1.2320us         2     616ns     282ns     950ns  cuDeviceGet\n",
      "                    0.00%     669ns         1     669ns     669ns     669ns  cuModuleGetLoadingMode\n",
      "                    0.00%     394ns         1     394ns     394ns     394ns  cuDeviceGetUuid\n"
     ]
    }
   ],
   "source": [
    "!nvprof ./matrixVectorScale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "owHmJDV4uF_V",
    "outputId": "0724dbbc-acff-45e7-b709-4f824e9f1400"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing nonlinear_elementwise_with_printf.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile nonlinear_elementwise_with_printf.cu\n",
    "\n",
    "#include <cuda.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "#include <math.h>\n",
    "\n",
    "// Example: Sigmoid function approximation using a Taylor series (for illustration)\n",
    "__device__ float sigmoid(float x) {\n",
    "    // Simple approximation: sigmoid(x) ≈ x / (1 + |x|)\n",
    "    return x / (1.0f + fabsf(x));\n",
    "}\n",
    "\n",
    "__global__ void applyNonLinearFunction(const float* input, float* output, int size) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if(idx < size) {\n",
    "        output[idx] = sigmoid(input[idx]);\n",
    "\n",
    "        // Print intermediate non-linear application results for debugging\n",
    "        printf(\"Thread %d: input[%d] = %.2f, sigmoid ≈ %.2f\\n\", idx, idx, input[idx], output[idx]);\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    // Example vector size\n",
    "    int size = 8;\n",
    "\n",
    "    // Allocate host memory\n",
    "    float h_input[] = {-3.0f, -1.0f, 0.0f, 1.0f, 2.0f, 3.0f, 4.0f, 5.0f};\n",
    "    float h_output[size] = {0};\n",
    "\n",
    "    // Allocate device memory\n",
    "    float *d_input, *d_output;\n",
    "    cudaMalloc((void**)&d_input, size * sizeof(float));\n",
    "    cudaMalloc((void**)&d_output, size * sizeof(float));\n",
    "\n",
    "    // Copy data to device\n",
    "    cudaMemcpy(d_input, h_input, size * sizeof(float), cudaMemcpyHostToDevice);\n",
    "\n",
    "    // Define block and grid sizes\n",
    "    int threadsPerBlock = 256;\n",
    "    int blocksPerGrid = (size + threadsPerBlock - 1) / threadsPerBlock;\n",
    "\n",
    "    // Launch kernel\n",
    "    applyNonLinearFunction<<<blocksPerGrid, threadsPerBlock>>>(d_input, d_output, size);\n",
    "\n",
    "    // Wait for GPU to finish\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    // Copy result back to host\n",
    "    cudaMemcpy(h_output, d_output, size * sizeof(float), cudaMemcpyDeviceToHost);\n",
    "\n",
    "    // Print the output vector after applying non-linear function\n",
    "    printf(\"\\nOutput Vector after Sigmoid Approximation:\\n\");\n",
    "    for(int i = 0; i < size; i++) {\n",
    "        printf(\"output[%d] = %.2f\\n\", i, h_output[i]);\n",
    "    }\n",
    "\n",
    "    // Cleanup\n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MoQROwmLuLdj",
    "outputId": "5262ae8b-59f6-4f59-e08b-f035814c00b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread 0: input[0] = -3.00, sigmoid ≈ -0.75\n",
      "Thread 1: input[1] = -1.00, sigmoid ≈ -0.50\n",
      "Thread 2: input[2] = 0.00, sigmoid ≈ 0.00\n",
      "Thread 3: input[3] = 1.00, sigmoid ≈ 0.50\n",
      "Thread 4: input[4] = 2.00, sigmoid ≈ 0.67\n",
      "Thread 5: input[5] = 3.00, sigmoid ≈ 0.75\n",
      "Thread 6: input[6] = 4.00, sigmoid ≈ 0.80\n",
      "Thread 7: input[7] = 5.00, sigmoid ≈ 0.83\n",
      "\n",
      "Output Vector after Sigmoid Approximation:\n",
      "output[0] = -0.75\n",
      "output[1] = -0.50\n",
      "output[2] = 0.00\n",
      "output[3] = 0.50\n",
      "output[4] = 0.67\n",
      "output[5] = 0.75\n",
      "output[6] = 0.80\n",
      "output[7] = 0.83\n"
     ]
    }
   ],
   "source": [
    "!nvcc nonlinear_elementwise_with_printf.cu -o nonlinear_elementwise_with_printf\n",
    "!./nonlinear_elementwise_with_printf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OFz8UAn5uOY5",
    "outputId": "006b172f-d0db-4adc-e4ea-328b325a2793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==2305== NVPROF is profiling process 2305, command: ./nonlinear_elementwise_with_printf\n",
      "Thread 0: input[0] = -3.00, sigmoid ≈ -0.75\n",
      "Thread 1: input[1] = -1.00, sigmoid ≈ -0.50\n",
      "Thread 2: input[2] = 0.00, sigmoid ≈ 0.00\n",
      "Thread 3: input[3] = 1.00, sigmoid ≈ 0.50\n",
      "Thread 4: input[4] = 2.00, sigmoid ≈ 0.67\n",
      "Thread 5: input[5] = 3.00, sigmoid ≈ 0.75\n",
      "Thread 6: input[6] = 4.00, sigmoid ≈ 0.80\n",
      "Thread 7: input[7] = 5.00, sigmoid ≈ 0.83\n",
      "\n",
      "Output Vector after Sigmoid Approximation:\n",
      "output[0] = -0.75\n",
      "output[1] = -0.50\n",
      "output[2] = 0.00\n",
      "output[3] = 0.50\n",
      "output[4] = 0.67\n",
      "output[5] = 0.75\n",
      "output[6] = 0.80\n",
      "output[7] = 0.83\n",
      "==2305== Profiling application: ./nonlinear_elementwise_with_printf\n",
      "==2305== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:   97.05%  110.46us         1  110.46us  110.46us  110.46us  applyNonLinearFunction(float const *, float*, int)\n",
      "                    1.91%  2.1760us         1  2.1760us  2.1760us  2.1760us  [CUDA memcpy DtoH]\n",
      "                    1.04%  1.1840us         1  1.1840us  1.1840us  1.1840us  [CUDA memcpy HtoD]\n",
      "      API calls:   96.55%  95.558ms         2  47.779ms  4.4330us  95.553ms  cudaMalloc\n",
      "                    2.72%  2.6937ms         1  2.6937ms  2.6937ms  2.6937ms  cudaLaunchKernel\n",
      "                    0.35%  348.86us         2  174.43us  11.130us  337.73us  cudaFree\n",
      "                    0.16%  156.95us         1  156.95us  156.95us  156.95us  cudaDeviceSynchronize\n",
      "                    0.14%  133.74us       114  1.1730us     144ns  51.947us  cuDeviceGetAttribute\n",
      "                    0.05%  49.410us         2  24.705us  20.322us  29.088us  cudaMemcpy\n",
      "                    0.01%  13.295us         1  13.295us  13.295us  13.295us  cuDeviceGetName\n",
      "                    0.01%  6.7340us         1  6.7340us  6.7340us  6.7340us  cuDeviceGetPCIBusId\n",
      "                    0.01%  5.1630us         1  5.1630us  5.1630us  5.1630us  cuDeviceTotalMem\n",
      "                    0.00%  1.6390us         3     546ns     227ns  1.1320us  cuDeviceGetCount\n",
      "                    0.00%  1.1860us         2     593ns     172ns  1.0140us  cuDeviceGet\n",
      "                    0.00%     479ns         1     479ns     479ns     479ns  cuModuleGetLoadingMode\n",
      "                    0.00%     272ns         1     272ns     272ns     272ns  cuDeviceGetUuid\n"
     ]
    }
   ],
   "source": [
    "!nvprof ./nonlinear_elementwise_with_printf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XF6cwr9kEwVL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlMMUlXeGLho"
   },
   "source": [
    "Use Thrust library to perform matrix-vector multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3-LQ7hiJ9MN"
   },
   "outputs": [],
   "source": [
    "#include <iostream>\n",
    "#include <thrust/device_vector.h>\n",
    "#include <thrust/transform.h>\n",
    "#include <thrust/sequence.h>\n",
    "\n",
    "struct matrix_vector_mult {\n",
    "    const float* matrix;\n",
    "    const float* vector;\n",
    "    int cols;\n",
    "\n",
    "    matrix_vector_mult(const float* m, const float* v, int c) : matrix(m), vector(v), cols(c) {}\n",
    "\n",
    "    __device__ float operator()(int row) {\n",
    "        float sum = 0;\n",
    "        for (int i = 0; i < cols; i++) {\n",
    "            sum += matrix[row * cols + i] * vector[i];\n",
    "        }\n",
    "        return sum;\n",
    "    }\n",
    "};\n",
    "\n",
    "int main() {\n",
    "    const int rows = 3, cols = 4;\n",
    "    float h_matrix[rows * cols] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12};\n",
    "    float h_vector[cols] = {1, 1, 1, 1};\n",
    "    thrust::device_vector<float> d_matrix(h_matrix, h_matrix + rows * cols);\n",
    "    thrust::device_vector<float> d_vector(h_vector, h_vector + cols);\n",
    "    thrust::device_vector<float> d_output(rows);\n",
    "\n",
    "    thrust::transform(thrust::counting_iterator<int>(0), thrust::counting_iterator<int>(rows), d_output.begin(),\n",
    "                      matrix_vector_mult(thrust::raw_pointer_cast(d_matrix.data()), thrust::raw_pointer_cast(d_vector.data()), cols));\n",
    "\n",
    "    std::cout << \"Thrust Matrix-Vector Multiplication Output:\\n\";\n",
    "    for (int i = 0; i < rows; i++) {\n",
    "        std::cout << d_output[i] << \" \";\n",
    "    }\n",
    "    std::cout << std::endl;\n",
    "\n",
    "    return 0;\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
