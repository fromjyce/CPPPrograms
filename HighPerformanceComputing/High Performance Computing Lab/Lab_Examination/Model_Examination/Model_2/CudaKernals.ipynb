{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0V8NnvmSKrFq"
   },
   "source": [
    "# Introduction to CUDA programming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVw_J3CaK1SI"
   },
   "source": [
    "### Check the cuda compiler version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nCyDEZzFQHam"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F9YjE75qKWqE"
   },
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fLAbQkLUPkEU"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/NVIDIA/cuda-samples.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B3EUu36vQQVc"
   },
   "outputs": [],
   "source": [
    "!cd cuda-samples/Samples/1_Utilities/deviceQuery && make\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oSYH_oRqQl8r"
   },
   "outputs": [],
   "source": [
    "!cd cuda-samples/Samples/1_Utilities/deviceQuery && ls\n",
    "!cuda-samples/Samples/1_Utilities/deviceQuery/./deviceQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ziDtLTyLArg"
   },
   "source": [
    "## nvcc for Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-G6_2BDjK4r8"
   },
   "outputs": [],
   "source": [
    "!pip install nvcc4jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYLqxQsBLKKg"
   },
   "outputs": [],
   "source": [
    "%load_ext nvcc4jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Questions\n",
    "\n",
    "1. Given a vector performs 1D convolution.\n",
    "2. Given a matrix, convert it to a vector and perform element-wise operations.\n",
    "3. Given a vector, find the elementwise application of non-linear functions through approximations.\n",
    "4. Given a vector, the first stage of stream compaction is performed. (using the boolean condition, convert the array to 0-1)\n",
    "5. Perfo4rm matrix-vector multiplication \n",
    "6. Perform reduction in a matrix (Convert matrix to vector)\n",
    "7. Using shared memory for matrix transpose\n",
    "8. Matrix-Matrix Multiplication Using a Standard CUDA Kernel with Flattened Matrices\n",
    "9. Use Thrust library to perform matrix-vector multiplication (self-study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: 1D Convolution on a Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cuda\n",
    "#include <stdio.h>\n",
    "\n",
    "__global__ void convolve1D(float *input, float *output, float *kernel, int inputSize, int kernelSize) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int halfKernel = kernelSize / 2;\n",
    "    if (idx >= inputSize) return;\n",
    "\n",
    "    float sum = 0;\n",
    "    for (int j = -halfKernel; j <= halfKernel; ++j) {\n",
    "        int index = idx + j;\n",
    "        if (index >= 0 && index < inputSize)\n",
    "            sum += input[index] * kernel[halfKernel + j];\n",
    "    }\n",
    "    output[idx] = sum;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int inputSize = 1024;\n",
    "    const int kernelSize = 5;\n",
    "\n",
    "    float *h_input = (float*)malloc(inputSize * sizeof(float));\n",
    "    float *h_output = (float*)malloc(inputSize * sizeof(float));\n",
    "    float h_kernel[kernelSize] = {0.2, 0.2, 0.2, 0.2, 0.2};\n",
    "\n",
    "    float *d_input, *d_output, *d_kernel;\n",
    "    cudaMalloc(&d_input, inputSize * sizeof(float));\n",
    "    cudaMalloc(&d_output, inputSize * sizeof(float));\n",
    "    cudaMalloc(&d_kernel, kernelSize * sizeof(float));\n",
    "\n",
    "    cudaMemcpy(d_input, h_input, inputSize * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_kernel, h_kernel, kernelSize * sizeof(float), cudaMemcpyHostToDevice);\n",
    "\n",
    "    int blockSize = 256;\n",
    "    int numBlocks = (inputSize + blockSize - 1) / blockSize;\n",
    "    convolve1D<<<numBlocks, blockSize>>>(d_input, d_output, d_kernel, inputSize, kernelSize);\n",
    "\n",
    "    cudaMemcpy(h_output, d_output, inputSize * sizeof(float), cudaMemcpyDeviceToHost);\n",
    "\n",
    "    cudaFree(d_input); cudaFree(d_output); cudaFree(d_kernel);\n",
    "    free(h_input); free(h_output);\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Matrix to Vector Conversion and Element-wise Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cuda\n",
    "#include <stdio.h>\n",
    "\n",
    "__global__ void matrixToVector(float *matrix, float *vector, int rows, int cols) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < rows * cols) {\n",
    "        vector[idx] = matrix[idx] * 2.0; // Example element-wise operation\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int rows = 1024;\n",
    "    const int cols = 1024;\n",
    "    const int size = rows * cols;\n",
    "\n",
    "    float *h_matrix = (float*)malloc(size * sizeof(float));\n",
    "    float *h_vector = (float*)malloc(size * sizeof(float));\n",
    "    float *d_matrix, *d_vector;\n",
    "\n",
    "    cudaMalloc(&d_matrix, size * sizeof(float));\n",
    "    cudaMalloc(&d_vector, size * sizeof(float));\n",
    "\n",
    "    cudaMemcpy(d_matrix, h_matrix, size * sizeof(float), cudaMemcpyHostToDevice);\n",
    "\n",
    "    int blockSize = 256;\n",
    "    int numBlocks = (size + blockSize - 1) / blockSize;\n",
    "    matrixToVector<<<numBlocks, blockSize>>>(d_matrix, d_vector, rows, cols);\n",
    "\n",
    "    cudaMemcpy(h_vector, d_vector, size * sizeof(float), cudaMemcpyDeviceToHost);\n",
    "\n",
    "    cudaFree(d_matrix); cudaFree(d_vector);\n",
    "    free(h_matrix); free(h_vector);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Given a vector, find the elementwise application of non-linear functions through approximations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cuda\n",
    "#include <stdio.h>\n",
    "#include <cuda.h>\n",
    "#include <math.h>\n",
    "\n",
    "#define N 10\n",
    "\n",
    "__device__ float approximate_tanh(float x) {\n",
    "    // Approximate tanh(x) using Taylor expansion for small values of x\n",
    "    return x - (x * x * x) / 3.0f + (2 * x * x * x * x * x) / 15.0f;\n",
    "}\n",
    "\n",
    "__global__ void apply_nonlinear(float *out, float *in, int n) {\n",
    "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    if (idx < n) {\n",
    "        out[idx] = approximate_tanh(in[idx]);\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    float in[N], out[N];\n",
    "    float *d_in, *d_out;\n",
    "\n",
    "    // Initialize input array with sample values\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        in[i] = i * 0.1f;  // for example: 0.0, 0.1, 0.2, ..., 0.9\n",
    "    }\n",
    "\n",
    "    // Allocate device memory\n",
    "    cudaMalloc((void**)&d_in, sizeof(float) * N);\n",
    "    cudaMalloc((void**)&d_out, sizeof(float) * N);\n",
    "\n",
    "    // Copy input to device\n",
    "    cudaMemcpy(d_in, in, sizeof(float) * N, cudaMemcpyHostToDevice);\n",
    "\n",
    "    // Launch kernel\n",
    "    apply_nonlinear<<<1, N>>>(d_out, d_in, N);\n",
    "\n",
    "    // Copy result back to host\n",
    "    cudaMemcpy(out, d_out, sizeof(float) * N, cudaMemcpyDeviceToHost);\n",
    "\n",
    "    // Print output\n",
    "    printf(\"Elementwise tanh approximation:\\n\");\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        printf(\"tanh(%f) ≈ %f\\n\", in[i], out[i]);\n",
    "    }\n",
    "\n",
    "    // Free device memory\n",
    "    cudaFree(d_in);\n",
    "    cudaFree(d_out);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Given a vector, the first stage of stream compaction is performed. (using the boolean condition, convert the array to 0-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cuda\n",
    "#include <stdio.h>\n",
    "#include <cuda.h>\n",
    "\n",
    "#define N 10\n",
    "\n",
    "__global__ void stream_compaction(int *out, float *in, int n) {\n",
    "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    if (idx < n) {\n",
    "        out[idx] = (in[idx] > 0.5f) ? 1 : 0;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    float in[N] = {0.1f, 0.4f, 0.6f, 0.7f, 0.2f, 0.9f, 0.3f, 0.5f, 0.8f, 0.15f};\n",
    "    int out[N];\n",
    "    float *d_in;\n",
    "    int *d_out;\n",
    "\n",
    "    // Allocate device memory\n",
    "    cudaMalloc((void**)&d_in, sizeof(float) * N);\n",
    "    cudaMalloc((void**)&d_out, sizeof(int) * N);\n",
    "\n",
    "    // Copy input to device\n",
    "    cudaMemcpy(d_in, in, sizeof(float) * N, cudaMemcpyHostToDevice);\n",
    "\n",
    "    // Launch kernel\n",
    "    stream_compaction<<<1, N>>>(d_out, d_in, N);\n",
    "\n",
    "    // Copy result back to host\n",
    "    cudaMemcpy(out, d_out, sizeof(int) * N, cudaMemcpyDeviceToHost);\n",
    "\n",
    "    // Print output\n",
    "    printf(\"Stream compaction (0-1 Boolean Conversion):\\n\");\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        printf(\"Input: %f -> Output: %d\\n\", in[i], out[i]);\n",
    "    }\n",
    "\n",
    "    // Free device memory\n",
    "    cudaFree(d_in);\n",
    "    cudaFree(d_out);\n",
    "\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Matrix-Vector Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cuda\n",
    "#include <stdio.h>\n",
    "\n",
    "__global__ void matVecMult(float *matrix, float *vector, float *result, int rows, int cols) {\n",
    "    int row = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (row < rows) {\n",
    "        float sum = 0.0f;\n",
    "        for (int col = 0; col < cols; ++col) {\n",
    "            sum += matrix[row * cols + col] * vector[col];\n",
    "        }\n",
    "        result[row] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int rows = 1024;\n",
    "    const int cols = 1024;\n",
    "\n",
    "    float *h_matrix = (float*)malloc(rows * cols * sizeof(float));\n",
    "    float *h_vector = (float*)malloc(cols * sizeof(float));\n",
    "    float *h_result = (float*)malloc(rows * sizeof(float));\n",
    "\n",
    "    float *d_matrix, *d_vector, *d_result;\n",
    "    cudaMalloc(&d_matrix, rows * cols * sizeof(float));\n",
    "    cudaMalloc(&d_vector, cols * sizeof(float));\n",
    "    cudaMalloc(&d_result, rows * sizeof(float));\n",
    "\n",
    "    cudaMemcpy(d_matrix, h_matrix, rows * cols * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_vector, h_vector, cols * sizeof(float), cudaMemcpyHostToDevice);\n",
    "\n",
    "    int blockSize = 256;\n",
    "    int numBlocks = (rows + blockSize - 1) / blockSize;\n",
    "    matVecMult<<<numBlocks, blockSize>>>(d_matrix, d_vector, d_result, rows, cols);\n",
    "\n",
    "    cudaMemcpy(h_result, d_result, rows * sizeof(float), cudaMemcpyDeviceToHost);\n",
    "\n",
    "    cudaFree(d_matrix); cudaFree(d_vector); cudaFree(d_result);\n",
    "    free(h_matrix); free(h_vector); free(h_result);\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Task 6: Perform reduction in a matrix (Convert matrix to vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cuda\n",
    "#include <stdio.h>\n",
    "#include <cuda.h>\n",
    "\n",
    "#define N 4  // Number of rows\n",
    "#define M 4  // Number of columns\n",
    "\n",
    "__global__ void reduce_matrix_to_vector(float *matrix, float *vector, int rows, int cols) {\n",
    "    int row = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (row < rows) {\n",
    "        float sum = 0.0;\n",
    "        for (int col = 0; col < cols; col++) {\n",
    "            sum += matrix[row * cols + col];\n",
    "        }\n",
    "        vector[row] = sum;  // Store the row sum in the vector\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    float h_matrix[N][M] = {\n",
    "        {1.0, 2.0, 3.0, 4.0},\n",
    "        {5.0, 6.0, 7.0, 8.0},\n",
    "        {9.0, 10.0, 11.0, 12.0},\n",
    "        {13.0, 14.0, 15.0, 16.0}\n",
    "    };\n",
    "    float h_vector[N];  // Vector to store row sums\n",
    "    float *d_matrix, *d_vector;\n",
    "\n",
    "    // Allocate device memory\n",
    "    cudaMalloc((void**)&d_matrix, sizeof(float) * N * M);\n",
    "    cudaMalloc((void**)&d_vector, sizeof(float) * N);\n",
    "\n",
    "    // Copy matrix to device\n",
    "    cudaMemcpy(d_matrix, h_matrix, sizeof(float) * N * M, cudaMemcpyHostToDevice);\n",
    "\n",
    "    // Launch kernel to reduce matrix to vector\n",
    "    reduce_matrix_to_vector<<<1, N>>>(d_matrix, d_vector, N, M);\n",
    "\n",
    "    // Copy result back to host\n",
    "    cudaMemcpy(h_vector, d_vector, sizeof(float) * N, cudaMemcpyDeviceToHost);\n",
    "\n",
    "    // Print output vector\n",
    "    printf(\"Row sums (reduction result):\\n\");\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        printf(\"Row %d sum: %f\\n\", i, h_vector[i]);\n",
    "    }\n",
    "\n",
    "    // Free device memory\n",
    "    cudaFree(d_matrix);\n",
    "    cudaFree(d_vector);\n",
    "\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Using shared memory for matrix transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cuda\n",
    "#include <stdio.h>\n",
    "\n",
    "#define TILE_WIDTH 32\n",
    "\n",
    "__global__ void matrixTranspose(float *input, float *output, int width, int height) {\n",
    "    __shared__ float tile[TILE_WIDTH][TILE_WIDTH];\n",
    "\n",
    "    int x = blockIdx.x * TILE_WIDTH + threadIdx.x;\n",
    "    int y = blockIdx.y * TILE_WIDTH + threadIdx.y;\n",
    "\n",
    "    if (x < width && y < height) {\n",
    "        int index_in = y * width + x;\n",
    "        tile[threadIdx.y][threadIdx.x] = input[index_in];\n",
    "    }\n",
    "\n",
    "    __syncthreads();\n",
    "\n",
    "    x = blockIdx.y * TILE_WIDTH + threadIdx.x;\n",
    "    y = blockIdx.x * TILE_WIDTH + threadIdx.y;\n",
    "\n",
    "    if (x < height && y < width) {\n",
    "        int index_out = y * height + x;\n",
    "        output[index_out] = tile[threadIdx.x][threadIdx.y];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int width = 1024;\n",
    "    const int height = 1024;\n",
    "\n",
    "    float *h_input = (float*)malloc(width * height * sizeof(float));\n",
    "    float *h_output = (float*)malloc(width * height * sizeof(float));\n",
    "    float *d_input, *d_output;\n",
    "\n",
    "    cudaMalloc(&d_input, width * height * sizeof(float));\n",
    "    cudaMalloc(&d_output, width * height * sizeof(float));\n",
    "\n",
    "    cudaMemcpy(d_input, h_input, width * height * sizeof(float), cudaMemcpyHostToDevice);\n",
    "\n",
    "    dim3 dimBlock(TILE_WIDTH, TILE_WIDTH);\n",
    "    dim3 dimGrid((width + TILE_WIDTH - 1) / TILE_WIDTH, (height + TILE_WIDTH - 1) / TILE_WIDTH);\n",
    "\n",
    "    matrixTranspose<<<dimGrid, dimBlock>>>(d_input, d_output, width, height);\n",
    "\n",
    "    cudaMemcpy(h_output, d_output, width * height * sizeof(float), cudaMemcpyDeviceToHost);\n",
    "\n",
    "    cudaFree(d_input); cudaFree(d_output);\n",
    "    free(h_input); free(h_output);\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: Matrix-Matrix Multiplication Using a Standard CUDA Kernel with Flattened Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cuda\n",
    "#include <stdio.h>\n",
    "#include <cuda.h>\n",
    "\n",
    "#define N 4  // Define matrix dimensions (N x N)\n",
    "\n",
    "__global__ void matrixMultiplyKernel(float *A, float *B, float *C, int n) {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (row < n && col < n) {\n",
    "        float sum = 0;\n",
    "        for (int i = 0; i < n; i++) {\n",
    "            sum += A[row * n + i] * B[i * n + col];\n",
    "        }\n",
    "        C[row * n + col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    float h_A[N * N] = {\n",
    "        1, 2, 3, 4,\n",
    "        5, 6, 7, 8,\n",
    "        9, 10, 11, 12,\n",
    "        13, 14, 15, 16\n",
    "    };\n",
    "    float h_B[N * N] = {\n",
    "        16, 15, 14, 13,\n",
    "        12, 11, 10, 9,\n",
    "        8, 7, 6, 5,\n",
    "        4, 3, 2, 1\n",
    "    };\n",
    "    float h_C[N * N];  // Output matrix\n",
    "\n",
    "    float *d_A, *d_B, *d_C;\n",
    "\n",
    "    // Allocate device memory\n",
    "    cudaMalloc((void**)&d_A, sizeof(float) * N * N);\n",
    "    cudaMalloc((void**)&d_B, sizeof(float) * N * N);\n",
    "    cudaMalloc((void**)&d_C, sizeof(float) * N * N);\n",
    "\n",
    "    // Copy matrices A and B to device\n",
    "    cudaMemcpy(d_A, h_A, sizeof(float) * N * N, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_B, h_B, sizeof(float) * N * N, cudaMemcpyHostToDevice);\n",
    "\n",
    "    // Define block and grid sizes\n",
    "    dim3 threadsPerBlock(16, 16);\n",
    "    dim3 blocksPerGrid((N + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
    "                       (N + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
    "\n",
    "    // Launch kernel for matrix-matrix multiplication\n",
    "    matrixMultiplyKernel<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);\n",
    "\n",
    "    // Copy result back to host\n",
    "    cudaMemcpy(h_C, d_C, sizeof(float) * N * N, cudaMemcpyDeviceToHost);\n",
    "\n",
    "    // Print result\n",
    "    printf(\"Resultant Matrix C:\\n\");\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        for (int j = 0; j < N; j++) {\n",
    "            printf(\"%f \", h_C[i * N + j]);\n",
    "        }\n",
    "        printf(\"\\n\");\n",
    "    }\n",
    "\n",
    "    // Free device memory\n",
    "    cudaFree(d_A);\n",
    "    cudaFree(d_B);\n",
    "    cudaFree(d_C);\n",
    "\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9: Use Thrust library to perform matrix-vector multiplication (self-study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cuda\n",
    "#include <thrust/host_vector.h>\n",
    "#include <thrust/device_vector.h>\n",
    "#include <thrust/transform.h>\n",
    "#include <thrust/functional.h>\n",
    "#include <iostream>\n",
    "\n",
    "#define N 4  // Dimension of matrix and vector\n",
    "\n",
    "struct matrix_vector_mult {\n",
    "    float *matrix;\n",
    "    int cols;\n",
    "\n",
    "    matrix_vector_mult(float *matrix, int cols) : matrix(matrix), cols(cols) {}\n",
    "\n",
    "    __host__ __device__ float operator()(int row_index, const float& vec_elem) {\n",
    "        float sum = 0.0;\n",
    "        for (int j = 0; j < cols; j++) {\n",
    "            sum += matrix[row_index * cols + j] * vec_elem;\n",
    "        }\n",
    "        return sum;\n",
    "    }\n",
    "};\n",
    "\n",
    "int main() {\n",
    "    float h_matrix[N * N] = {\n",
    "        1, 2, 3, 4,\n",
    "        5, 6, 7, 8,\n",
    "        9, 10, 11, 12,\n",
    "        13, 14, 15, 16\n",
    "    };\n",
    "    float h_vector[N] = {1, 2, 3, 4};  // Vector for multiplication\n",
    "    thrust::host_vector<float> h_result(N);\n",
    "\n",
    "    thrust::device_vector<float> d_matrix(h_matrix, h_matrix + N * N);\n",
    "    thrust::device_vector<float> d_vector(h_vector, h_vector + N);\n",
    "    thrust::device_vector<float> d_result(N);\n",
    "\n",
    "    // Perform matrix-vector multiplication using transform\n",
    "    thrust::transform(thrust::counting_iterator<int>(0),\n",
    "                      thrust::counting_iterator<int>(N),\n",
    "                      d_result.begin(),\n",
    "                      matrix_vector_mult(thrust::raw_pointer_cast(d_matrix.data()), N));\n",
    "\n",
    "    // Copy result back to host and print\n",
    "    thrust::copy(d_result.begin(), d_result.end(), h_result.begin());\n",
    "\n",
    "    std::cout << \"Resultant Vector:\\n\";\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        std::cout << h_result[i] << \" \";\n",
    "    }\n",
    "    std::cout << std::endl;\n",
    "\n",
    "    return 0;\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
